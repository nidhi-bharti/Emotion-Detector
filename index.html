<html>
	<head>
		<style type="text/css">
			body{font-family: sans-serif;}
			p{display: inline-block;}
			table{
				font-size: 12px;
				width: 100%;
				border-collapse: collapse;
    		border: 1px solid black;
			}
			tr, td{
				border: 1px solid black;
			}
			img{
				align-items: center;
				width: 80%;
				display: block;
			}
			.container{width: 90%;position absolute;margin: auto;}
			.title{position: relative;width: 90%;margin: auto;text-align: center;font-weight: bold;font-size: 24px;padding: 1%;}
			.section{position: relative;width: 90%;margin: auto;padding: 2%;}
			.subsection{position: relative; width: 98%;text-align: justify;padding: 10px;}
			.heading{position: relative; width: 98%;text-align: left;font-size: 18px;font-weight: bold;}
			.text{width: 95%;font-size: 14px;text-align: justify;padding: 10px 0px 10px 0px;}
			.authors{position: relative;width: 80%;margin: auto;padding: 2%;font-style: italic;text-align: center;font-size: 12px;}
			.image{width: 95%;font-size: 14px;text-align: left;}
		</style>
	</head>
	<body>
		<div class="container">
			<div class="title">EMOTION DETECTOR</div>

			<div class="authors">

				<!-- Start edit here  -->
				<p>Nidhi Bharti, Roll No.: 150102040, Branch: ECE;</p>; &nbsp; &nbsp;
				<p>Pratiksha Ramesh Bhere, Roll No.: 150102053, Branch: ECE;</p>; &nbsp; &nbsp;
				<p>Tumati Likhitha, Roll No.: 150102067, Branch: ECE; </p>; &nbsp; &nbsp;
				<p>Divyangi Gatwar, Roll No.: 150102084, Branch: ECE;</p>; &nbsp; &nbsp;
				<p>Dungarwal Rutuja Pravin, Roll No.: 150108013, Branch: EEE</p>; &nbsp; &nbsp;
				<!-- Stop edit here -->

			</div>


			<div class="section">
				<div class="heading">Abstract</div>
				<div class="text">

					<!-- Start edit here  -->
					The project aims at detecting human emotion expressed in an audio clip via speech recognition using four vital characteristic features of a speech signal. These are MFCC, VAD, pitch,and formants. The proposed methodology classifies the audio clip as one of the three categories, namely, “JOY”,”ANGER” and “NEUTRAL” .Multi-Class SVM is used for classification purpose. The algorithm is trained and tested using Berlin emotional Database.

				</div>
			</div>

			<div class="section">
				<div class="heading">1. Introduction</div>
				<div class="text">

					<!-- Start edit here  -->

										<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="heading">1.1 Introduction to Problem</div>
					<div class="text">

						<!-- Start edit here  -->
						Estimation of human emotions using a computer has been difficult since when human engaged in a conversational secession. Humans encode their emotions while communicating. Different human speech signals containing the same emotion may vary from person to person. Recognising the similarities in expression of same emotion by different people is certainly a challenging task.
						 <!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading">1.2 Figure</div>
					<div class="image">

						<!-- Start edit here  -->
						<!-- All images must be put in a Pictures folder. An example image -->
						 Block diagram of the system.
						<img src="Pictures/fig1.png" alt="This text displays when the image is umavailable" width="300px" height=""/>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.3 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						Applications of emotion classification based on speech have already been used to facilitate interactions in our daily lives. For example, At cash counters apply emotion classification to prioritize impatient customers. As another example, a warning system has been developed to detect if a driver exhibits anger or aggressive emotions. Emotion sensing has also been used in behaviour studies. Acoustic features have been extensively explored in both the time domain (energy, speaking rate, duration of voiced segments, zero crossing rate, etc.) and the frequency domain (pitch, formant, Mel-frequency cepstral coefficients, etc.). In our work, we only choose the most basic features: pitch, formants, and MFCC. This reduces the computational complexity of the approach and can lead to both energy and bandwidth savings when the voice is captured on mobile devices.
<br><br>
 Commonly used classifiers for human emotion recognition from speech such as Hidden Markov Model (HMM), Neural Network (NN), Maximum likelihood bayes classifier (MLBC), Kernel deterioration and K-nearest Neighbours approach (KNN), support vector machine (SVM), Naive Bayes classifier, Gaussian Mixture Model (GMM). We chose SVM as our basic classifier because of its ease of training and its ability to work with any number of attributes. SVM is simple and efficient algorithm which has a very good classification performance compared to other classifiers. SVM are the popular learning method for classification ,regression and other learning tasks. SVM has a better classification performance on a small amount of training samples. But we are lacking in guidelines on choosing a better kernel with optimized parameters of SVM.There is no uniform pattern used to the choice of SVM with its parameters and kernel function with its parameters.In SVM, kernel functions are used to map data to a higher dimensional feature space without losing the originality. This conventional method of using kernel functions in SVM is to run simulations on training sets and find the kernel function which attains the highest averaged classification accuracy for the given problem. The most commonly used kernel function for SVM is Linear, Polynomial, radial basis function (RBF).
<br><br>
The contributions of the Speech emotion recognition are as follows:

					<ul>
						<li>To obtain the maximum efficiency using the performance of SVM kernel method for each individual technique</li>
						<li>Consideration of a cut-off value in each technique so the classification having better confidence level is selected and those with lesser confidence value are discarded as ‘not classified’.</li>

					 </ul>
					 The major principle of SVM is to establish a hyperplane as the decision surface maximizing the margin of separation between negative and positive samples. Thus SVM is designed for two-class pattern classification. We have used German speech database in this approach of emotion recognition and classification. The accuracy of emotion recognition can be made better by increasing the value of minimum confidence cut-off value.
					 <br>
						<br>Framing: In Framing, the continuous input speech is segmented into N sample per frames. The first frame consists of N samples, second frame consists of M samples after N, and third frame contains 2M and so on. Here we frame the signal with time length of 25ms.


						<br>Windowing: Windowing is used to window each individual frame in order to remove the discontinuities at the start and end of the frame. Hamming window is mostly used due to its relatively narrow main lobe width hence, remove distortion.

						<br>Fast Fourier Transform: FFT algorithm is used for converting the N samples from time domain to frequency domain. It is used to evaluate frequency spectrum of speech
						<!-- Write something here. -->
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.4 Proposed Algorithm</div>
					<div class="text">

						<!-- Start edit here  -->
						<img src="Pictures/fig2.png" alt="This text displays when the image is umavailable" width="300px" height=""/>


						<!-- Describe your approach briefly here. -->
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.5 Report Organization</div>
					<div class="text">

						<!-- Start edit here  -->
						The rest of the report is organized as follows:
						<ul>


							<li>Section 2 contains the proposed approach and its elaborative description.</li>
							<li>Section 3 includes the experiments conducted on Berlin Emotion Database and results signifying % accuracy.</li>
							<li>Finally, Section 4 draws important conclusion and briefly discusses the future possibilities and extensions for the proposed algorithm.</li>
					</ul>
						<!-- Write something here. -->
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">2. Proposed Approach</div>
				<div class="text">

					<!-- Start edit here  -->
					STEP1: Extracting speech emotion feature from utterances.
					<br>STEP2: The main task in optimized process is to improve the classification accuracy rate of the SVM.
					<br>STEP3: After optimizing process, the system trains an optimized model used to classify.
					<br>STEP4: The system gives a classification result (class label or recognition rate) about test samples

				<ol type="A">
					<li> Speech features</li>
					In speech signal there are basically speech attributes for the feature extraction process. There
					are some important feature in speech signal which are pitch(F0), log-energy, formant frequency, mel-band energy and mel-frequency cepstral coefficient. More details of these features can be described as follows.
					<ol>
						<li> Frequency or pitch: A pitch is the fundamental frequency (F0) of the speech signal. It refers to the tone of speech. The pitch of each signal is different depends on the speed of the vibration of the object.</li>
						<li> Spectral features: Two types of spectral features: The Mel-Frequency Cepstral Coefficients (MFCCs) and formant are reported as effective spectral features for emotion recognition. Most of researcher used the first 12 MFCCs and 4 formants are extracted from 25 ms Hamming-windowed speech frames every 10 ms, and so their contours are formed.</li>
						<img src="Pictures/fig3.png" alt="This text displays when the image is umavailable" width="100px" height="">
						In order to obtain a set of MFCC features, the following procedure is commonly applied: Estimation of the spectrum by means of a Fourier transformation of the windowed signal, application of a triangular Mel filter bank to the estimated spectrum  and, finally, application of the DCT to the log-outputs of the filterbank.

				<li> VAD : Voice activity detection (VAD), also known as speech activity detection or speech detection, is a technique used in speech processing in which the presence or absence of human speech is detected.The main uses of VAD are in speech coding and speech recognition. Generally, it is language independent.</li>




					</ol>
					<br>
							<li>Feature extraction</li>
							In our project, we have mainly focussed on extraction of four important features -
							<ul>
							<li>MFCC</li>
							<li>VAD</li>
							<li>Pitch</li>
							<li>Formants</li>

						</ul>
							<br><li>SVM Classification</li>
							The underlying concept behind an SVM is structural risk minimization . A learning machine is chosen that minimizes the upper bound on the risk (or test error), which is a good measure of the generalizability of the machine. This is estimated as the ratio of misclassified vectors over the total number of training vectors when using a “leave-one-out” method . It can be shown that this is equal to the ratio of expected number of support vectors to the total number of training vectors.
The power of SVMs lies in transforming data to a high dimensional space and constructing a linear binary classifier in this high dimensional space. Construction of a hyperplane in a feature space requires transformation of the -dimensional input vector into an -dimensional feature vector, i.e.
              Φ: ℜn  --ℜN
 An -dimensional linear separator and a bias b are then constructed for the set of transformed vectors. Classification of an unknown vector is done by first transforming the vector to the feature space and then computing
             sgn(  w ⋅ φ(x ) + b )
The vector can be written as a linear combination of a small set of vectors in the feature space. This can be mathematically expressed as
        w=  ∑ αi .y.i φ( xi )
where the summation is over all vectors in the training set whose corresponding ‘s are non-zero. These vectors are called support vectors [2]. Combining Equations 2 and 3, the classifier becomes .
sgn∑ yi .αi  ⋅ φ(x) φ( xi )+b
      Svs

 the equation is prohibitively expensive to implement directly in the feature space, since it would involve a dot product computation in a very high dimensional space. However if we could define a function in the input space which equals the dot product in the feature space, the overall complexity of the process can be drastically reduced since we significantly lower the dimensionality. The existence of such a function is guaranteed by Mercer’s conditions [2,6]. Such functions are referred to as a kernel in the SVM approach. Thus Equation 4 can be   equivalently written as
sgn∑ yi .αi  ⋅ k(x,xi )+b
2 layer NN K(u v,) = Sigmoid s[ (uv)+c]

: RBF: K(u v )=  exp{y(u-v)(u-v)}
Since the decision region is dependent on the data set, by using prior knowledge of the data and the characteristics of various kernels, we can achieve better performance. For example, if a data set is known to need closed decision regions, it is better to use an RBF kernel rather than a linear or a low order polynomial kernel

					</ol>

					<!-- Write something here. -->
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading">3. Experiments &amp; Results</div>
				<div class="subsection">
					<div class="heading">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						We have used Berlin Emotional Data-base both for training and testing the algorithm. A genuine reason behind the same is Berlin Data-base is easily available as compared to other data sets, widely disseminated and used. This is a German speech database of about 800 sentences read by 10 actors with simulated emotion. Speakers (5 female and 5 male) were asked to read 10 utterances (5 short and 5 longer sentences) simulating different kinds of emotion: happiness, anger or no emotion at all (neutral). Recordings (wav format) were made in an anechoic room, at the sampling rate of 16 kHz and resolution of 16 bit. The total recording time per speaker was approximately of 2 hours.
						<br>
						For our purpose, we have divided the above dataset in two parts. The first part constitutes our training set containing approximately 78% of the total .wav files. The second part ,which is used for testing the algorithm and constitutes the test set, contains 21% of the total files.

						<!-- Write something here. -->
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">3.2 Discussion</div>
					<div class="text">

						<!-- Start edit here  -->
						As mentioned already, we have used the latest version of MATLAB 2017Rb as our coding platform. The experimental results on the test set are summarised in the following table.
						<!-- MAKE TABLE -->
						<br>
						<table>
							<tr>
								<td colspan="1">Emotion Expressed</td>
								<td colspan="3">Emotion Recognised (in%)</td>
							</tr>
							<tr>
								<td ></td>
								<td >JOY</td>
								<td>ANGER</td>
								<td >NEUTRAL</td>
							</tr>
							<tr>
								<td >JOY</td>
								<td >80.769</td>
								<td >11.538</td>
								<td >7.692</td>
							</tr>
							<tr>
								<td >Anger</td>
								<td >20.000</td>
								<td >80.000</td>
								<td >0.000</td>
							</tr>
							<tr>
								<td >Neutral</td>
								<td >28.571</td>
								<td >0.000</td>
								<td>71.428</td>
							</tr>
						</table>
						<!-- <p style = " text-align:center">Table 1</p> -->
						<br>
						The overall accuracy when tested over test set was 78.947%, i.e. the error percentage was 21.052% for the chosen test set. From the above table, it is noticeable that both the emotions (Anger and Neutral) have characteristics similar to that of Joy but do not resemble each other. The worst performance is given by the algorithm on Neutral test set with error % upto 28.571%, whereas the response of algorithm to Joy test set is the best with error % of just 19.5%.

						<br>Though it was expected that the Anger emotion would attain the highest accuracy ( as it is expressed with very different pitch as compared to the other two emotions), it stands at second position with minute difference of error % when compared with Joy emotion.

						<!-- Write something here. -->
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">4. Conclusions</div>
				<div class="subsection">
					<div class="heading">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						As technology evolves, interest in human like machines increases. Technological devices are spreading and user satisfaction increases importance. A natural interface which responds according to user needs has become possible with affective computing. The key issue of affective computing is emotions. Any research which is related with detection, recognition or generating an emotion is affective computing. User satisfaction or dissatisfaction could be detected with any emotion recognition system. Besides detection of user satisfaction, such systems could be used to detect anger or frustration. This can be used in providing benefits to the orators, in call centers, to act on one’s emotion interface in a situation, making of a humanoid robot etc. In such cases, user could be restrained like driving a car. In emotion detection tasks, speech or face emotion detections are the most popular ones. Easy access to face or speech data made them very popular. Speech carries a rich set of data. In human to human communication, via speech information is conveyed. Acoustic part of speech carries important info about emotions.MFCC are used for the feature extraction .Algorithm with the SVM overall performance is tested.
						<!-- Write something here. -->
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						We can use Neural Network instead of SVM to perform the classification on a larger training set. The Neural Networks can adapt to the new datasets aptly since there are hidden layers it allows the algorithm to be flexible and classify better which is the case with most artificial neural network to make it more human like.Time-domain features (TDFs) provide higher classification efficiencies than frequency-domain features (FDFs). Hence we can use the time domain features using neural networks. Furthermore, we can use Gaussian and Hidden Markov Model instead of Linear SVM to examine variety of signals. More features can be used to classify better.
						<!-- Write something here. -->
						<!-- Stop edit here -->

					</div>
				</div>
			</div>
			<p style="font-family:sans-serif , font-size:5px">Reference:Detection of Human Emotion via Speech Recognition by using Speech Spectogram</p>
			<br><p style="font-family:sans-serif,font:size=5px">Link to matlab code:<a href="https://github.com/nidhi-bharti/Emotion-detector/tree/master/Speech-Emotion-Recognition-master"> Click Here </a></p>
			
		</div>

	</body>
</html>
